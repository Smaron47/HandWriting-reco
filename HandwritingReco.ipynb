{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 11929,
     "status": "error",
     "timestamp": 1737437038828,
     "user": {
      "displayName": "Manik Chandra Biswas",
      "userId": "00149848136148089105"
     },
     "user_tz": -360
    },
    "id": "5rIMBSo-Rmfu"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import kagglehub\n",
    "\n",
    "# Download the latest dataset version\n",
    "path = kagglehub.dataset_download(\"nibinv23/iam-handwriting-word-database\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "# Constants\n",
    "IMAGE_SIZE = (128, 132)\n",
    "BATCH_SIZE = 20\n",
    "EPOCHS = 75\n",
    "PADDING_TOKEN = 99\n",
    "# DATA_INPUT_PATH = \"/content/drive/MyDrive/iam-handwriting-word-database\"\n",
    "DATA_INPUT_PATH = os.path.join(path, \"iam_words\")\n",
    "# Initialize variables\n",
    "images_path = []\n",
    "labels = []\n",
    "\n",
    "# Function to preprocess the dataset\n",
    "def preprocess_dataset():\n",
    "    characters = set()\n",
    "    max_len = 0\n",
    "    with open(os.path.join(DATA_INPUT_PATH, 'words.txt'), 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        print(lines)\n",
    "        for line_number, line in enumerate(lines):\n",
    "            if line.startswith('#') or line.strip() == '':\n",
    "                continue\n",
    "\n",
    "            parts = line.strip().split()\n",
    "            word_id = parts[0]\n",
    "            first_folder = word_id.split(\"-\")[0]\n",
    "            second_folder = first_folder + '-' + word_id.split(\"-\")[1]\n",
    "            image_filename = f\"{word_id}.png\"\n",
    "            image_path = os.path.join(DATA_INPUT_PATH, 'iam_words', 'words', first_folder, second_folder, image_filename)\n",
    "            print(word_id)\n",
    "            if os.path.isfile(image_path) and os.path.getsize(image_path):\n",
    "                images_path.append(image_path)\n",
    "                label = parts[-1].strip()\n",
    "                for char in label:\n",
    "                    characters.add(char)\n",
    "                max_len = max(max_len, len(label))\n",
    "                labels.append(label)\n",
    "        print(characters)\n",
    "\n",
    "    characters = sorted(list(characters))\n",
    "    char_to_num = tf.keras.layers.StringLookup(vocabulary=list(characters), mask_token=None)\n",
    "    num_to_char = tf.keras.layers.StringLookup(vocabulary=char_to_num.get_vocabulary(), mask_token=None, invert=True)\n",
    "    return characters, char_to_num, num_to_char, max_len\n",
    "\n",
    "characters, char_to_num, num_to_char, max_len = preprocess_dataset()\n",
    "\n",
    "# Function for distortion-free image resizing\n",
    "def distortion_free_resize(image, img_size):\n",
    "    w, h = img_size\n",
    "    image = tf.image.resize(image, size=(h, w), preserve_aspect_ratio=True)\n",
    "    pad_height = h - tf.shape(image)[0]\n",
    "    pad_width = w - tf.shape(image)[1]\n",
    "    pad_height_top, pad_height_bottom = divmod(pad_height, 2)\n",
    "    pad_width_left, pad_width_right = divmod(pad_width, 2)\n",
    "    image = tf.pad(image, paddings=[[pad_height_top, pad_height_bottom], [pad_width_left, pad_width_right], [0, 0]])\n",
    "    image = tf.transpose(image, perm=[1, 0, 2])\n",
    "    image = tf.image.flip_left_right(image)\n",
    "    return image\n",
    "\n",
    "# Image and label preprocessing functions\n",
    "def preprocess_image(image_path, img_size):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_png(image, 1)\n",
    "    image = distortion_free_resize(image, img_size)\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    return image\n",
    "\n",
    "def vectorize_label(label):\n",
    "    label = char_to_num(tf.strings.unicode_split(label, input_encoding=\"UTF-8\"))\n",
    "    length = tf.shape(label)[0]\n",
    "    pad_amount = max_len - length\n",
    "    label = tf.pad(label, paddings=[[0, pad_amount]], constant_values=PADDING_TOKEN)\n",
    "    return label\n",
    "\n",
    "def process_images_labels(image_path, label):\n",
    "    image = preprocess_image(image_path, IMAGE_SIZE)\n",
    "    label = vectorize_label(label)\n",
    "    return {\"image\": image, \"label\": label}\n",
    "\n",
    "# Prepare dataset\n",
    "def prepare_dataset(image_paths, labels):\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels)).map(process_images_labels, num_parallel_calls=AUTOTUNE)\n",
    "    return dataset.batch(BATCH_SIZE).cache().prefetch(AUTOTUNE)\n",
    "\n",
    "def split_dataset():\n",
    "    train_images, test_images, train_labels, test_labels = train_test_split(images_path, labels, test_size=0.2, random_state=42)\n",
    "    val_images, test_images, val_labels, test_labels = train_test_split(test_images, test_labels, test_size=0.5, random_state=42)\n",
    "    train_set = prepare_dataset(train_images, train_labels)\n",
    "    val_set = prepare_dataset(val_images, val_labels)\n",
    "    test_set = prepare_dataset(test_images, test_labels)\n",
    "    return train_set, val_set, test_set\n",
    "\n",
    "train_set, val_set, test_set = split_dataset()\n",
    "\n",
    "# CTC Layer for loss calculation\n",
    "class CTCLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.loss_fn = tf.keras.backend.ctc_batch_cost\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
    "        input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
    "        label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
    "        input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "        label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "        loss = self.loss_fn(y_true, y_pred, input_length, label_length)\n",
    "        self.add_loss(loss)\n",
    "        return y_pred\n",
    "\n",
    "# Model building function\n",
    "def build_model():\n",
    "    input_img = tf.keras.Input(shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 1), name=\"image\")\n",
    "    labels = tf.keras.layers.Input(name=\"label\", shape=(None,))\n",
    "    x = tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(input_img)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "    x = tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\", kernel_initializer=\"he_normal\", padding=\"same\")(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "    new_shape = ((IMAGE_SIZE[0] // 4), (IMAGE_SIZE[1] // 4) * 64)\n",
    "    x = tf.keras.layers.Reshape(target_shape=new_shape)(x)\n",
    "    x = tf.keras.layers.Dense(64, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True, dropout=0.25))(x)\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True, dropout=0.25))(x)\n",
    "    x = tf.keras.layers.Dense(len(char_to_num.get_vocabulary()) + 2, activation=\"softmax\", name=\"dense2\")(x)\n",
    "    output = CTCLayer(name=\"ctc_loss\")(labels, x)\n",
    "    model = tf.keras.models.Model(inputs=[input_img, labels], outputs=output)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "\n",
    "# Save the model to Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "model.save('/content/drive/MyDrive/handwriting_recognition_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xNr8y0Ib48eI"
   },
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras.models import Model\n",
    "\n",
    "from mltu.tensorflow.model_utils import residual_block\n",
    "\n",
    "\n",
    "def train_model(input_dim, output_dim, activation=\"leaky_relu\", dropout=0.2):\n",
    "\n",
    "    inputs = layers.Input(shape=input_dim, name=\"input\")\n",
    "\n",
    "    # normalize images here instead in preprocessing step\n",
    "    input = layers.Lambda(lambda x: x / 255)(inputs)\n",
    "\n",
    "    x1 = residual_block(input, 16, activation=activation, skip_conv=True, strides=1, dropout=dropout)\n",
    "\n",
    "    x2 = residual_block(x1, 16, activation=activation, skip_conv=True, strides=2, dropout=dropout)\n",
    "    x3 = residual_block(x2, 16, activation=activation, skip_conv=False, strides=1, dropout=dropout)\n",
    "\n",
    "    x4 = residual_block(x3, 32, activation=activation, skip_conv=True, strides=2, dropout=dropout)\n",
    "    x5 = residual_block(x4, 32, activation=activation, skip_conv=False, strides=1, dropout=dropout)\n",
    "\n",
    "    x6 = residual_block(x5, 64, activation=activation, skip_conv=True, strides=2, dropout=dropout)\n",
    "    x7 = residual_block(x6, 64, activation=activation, skip_conv=True, strides=1, dropout=dropout)\n",
    "\n",
    "    x8 = residual_block(x7, 64, activation=activation, skip_conv=False, strides=1, dropout=dropout)\n",
    "    x9 = residual_block(x8, 64, activation=activation, skip_conv=False, strides=1, dropout=dropout)\n",
    "\n",
    "    squeezed = layers.Reshape((x9.shape[-3] * x9.shape[-2], x9.shape[-1]))(x9)\n",
    "\n",
    "    blstm = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(squeezed)\n",
    "    blstm = layers.Dropout(dropout)(blstm)\n",
    "\n",
    "    output = layers.Dense(output_dim + 1, activation=\"softmax\", name=\"output\")(blstm)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 113474,
     "status": "error",
     "timestamp": 1737196815838,
     "user": {
      "displayName": "Manik Chandra Biswas",
      "userId": "00149848136148089105"
     },
     "user_tz": -360
    },
    "id": "Y8gc3D295LY3",
    "outputId": "dd070603-b0a6-4e3d-8279-bc9452b471b2"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check if GPU is available\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    print(\"GPU is not available\")\n",
    "\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "\n",
    "from mltu.preprocessors import ImageReader\n",
    "from mltu.transformers import ImageResizer, LabelIndexer, LabelPadding, ImageShowCV2\n",
    "from mltu.augmentors import RandomBrightness, RandomRotate, RandomErodeDilate, RandomSharpen\n",
    "from mltu.annotations.images import CVImage\n",
    "\n",
    "from mltu.tensorflow.dataProvider import DataProvider\n",
    "from mltu.tensorflow.losses import CTCloss\n",
    "from mltu.tensorflow.callbacks import Model2onnx, TrainLogger\n",
    "from mltu.tensorflow.metrics import CWERMetric\n",
    "\n",
    "\n",
    "# from configs import ModelConfigs\n",
    "\n",
    "import os\n",
    "import tarfile\n",
    "from tqdm import tqdm\n",
    "from urllib.request import urlopen\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set the path where you want to save the files on Google Drive\n",
    "drive_path = '/content/drive/MyDrive/MLTU_Models'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(drive_path):\n",
    "    os.makedirs(drive_path)\n",
    "\n",
    "\n",
    "def download_and_unzip(url, extract_to=\"Datasets\", chunk_size=1024*1024):\n",
    "    http_response = urlopen(url)\n",
    "\n",
    "    data = b\"\"\n",
    "    iterations = http_response.length // chunk_size + 1\n",
    "    for _ in tqdm(range(iterations)):\n",
    "        data += http_response.read(chunk_size)\n",
    "\n",
    "    zipfile = ZipFile(BytesIO(data))\n",
    "    zipfile.extractall(path=extract_to)\n",
    "\n",
    "dataset_path = os.path.join(\"Datasets\", \"IAM_Words\")\n",
    "if not os.path.exists(dataset_path):\n",
    "    download_and_unzip(\"https://git.io/J0fjL\", extract_to=\"Datasets\")\n",
    "\n",
    "    file = tarfile.open(os.path.join(dataset_path, \"words.tgz\"))\n",
    "    file.extractall(os.path.join(dataset_path, \"words\"))\n",
    "\n",
    "dataset, vocab, max_len = [], set(), 0\n",
    "\n",
    "# Preprocess the dataset by the specific IAM_Words dataset file structure\n",
    "words = open(os.path.join(dataset_path, \"words.txt\"), \"r\").readlines()\n",
    "for line in tqdm(words):\n",
    "    if line.startswith(\"#\"):\n",
    "        continue\n",
    "\n",
    "    line_split = line.split(\" \")\n",
    "    if line_split[1] == \"err\":\n",
    "        continue\n",
    "\n",
    "    folder1 = line_split[0][:3]\n",
    "    folder2 = \"-\".join(line_split[0].split(\"-\")[:2])\n",
    "    file_name = line_split[0] + \".png\"\n",
    "    label = line_split[-1].rstrip(\"\\n\")\n",
    "\n",
    "    rel_path = os.path.join(dataset_path, \"words\", folder1, folder2, file_name)\n",
    "    if not os.path.exists(rel_path):\n",
    "        print(f\"File not found: {rel_path}\")\n",
    "        continue\n",
    "\n",
    "    dataset.append([rel_path, label])\n",
    "    vocab.update(list(label))\n",
    "    max_len = max(max_len, len(label))\n",
    "\n",
    "# Create a ModelConfigs object to store model configurations\n",
    "configs = ModelConfigs()\n",
    "\n",
    "# Save vocab and maximum text length to configs\n",
    "configs.vocab = \"\".join(vocab)\n",
    "configs.max_text_length = max_len\n",
    "configs.save()\n",
    "\n",
    "# Create a data provider for the dataset\n",
    "data_provider = DataProvider(\n",
    "    dataset=dataset,\n",
    "    skip_validation=True,\n",
    "    batch_size=configs.batch_size*8,\n",
    "    data_preprocessors=[ImageReader(CVImage)],\n",
    "    transformers=[\n",
    "        ImageResizer(configs.width, configs.height, keep_aspect_ratio=False),\n",
    "        LabelIndexer(configs.vocab),\n",
    "        LabelPadding(max_word_length=configs.max_text_length, padding_value=len(configs.vocab)),\n",
    "        ],\n",
    ")\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_data_provider, val_data_provider = data_provider.split(split = 0.9)\n",
    "\n",
    "# Augment training data with random brightness, rotation and erode/dilate\n",
    "train_data_provider.augmentors = [\n",
    "    RandomBrightness(),\n",
    "    RandomErodeDilate(),\n",
    "    RandomSharpen(),\n",
    "    RandomRotate(angle=10),\n",
    "    ]\n",
    "\n",
    "# Creating TensorFlow model architecture\n",
    "model = train_model(\n",
    "    input_dim = (configs.height, configs.width, 3),\n",
    "    output_dim = len(configs.vocab),\n",
    ")\n",
    "\n",
    "# Compile the model and print summary\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=configs.learning_rate),\n",
    "    loss=CTCloss(),\n",
    "    metrics=[CWERMetric(padding_token=len(configs.vocab))],\n",
    ")\n",
    "model.summary(line_length=110)\n",
    "!pip install tf2onnx\n",
    "# Define callbacks\n",
    "# Define callbacks\n",
    "earlystopper = EarlyStopping(monitor=\"val_CER\", patience=20, verbose=1, mode='min') # Added mode='min'\n",
    "checkpoint = ModelCheckpoint(f\"{configs.model_path}/model.keras\", monitor=\"val_CER\", verbose=1, save_best_only=True, mode=\"min\")\n",
    "trainLogger = TrainLogger(configs.model_path)\n",
    "tb_callback = TensorBoard(f\"{configs.model_path}/logs\", update_freq=1)\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor=\"val_CER\", factor=0.9, min_delta=1e-10, patience=10, verbose=1, mode=\"auto\")\n",
    "model2onnx = Model2onnx(f\"{configs.model_path}/model.keras\")\n",
    "# Train the model\n",
    "train_data_provider.to_csv(os.path.join(configs.model_path, \"train.csv\"))\n",
    "val_data_provider.to_csv(os.path.join(configs.model_path, \"val.csv\"))\n",
    "model.fit(\n",
    "    train_data_provider,\n",
    "    validation_data=val_data_provider,\n",
    "    epochs=configs.train_epochs,\n",
    "    callbacks=[earlystopper, checkpoint, trainLogger, reduceLROnPlat, tb_callback, model2onnx],\n",
    "    # workers=configs.train_workers\n",
    ")\n",
    "\n",
    "# Save training and validation datasets as csv files\n",
    "\n",
    "# Save the model in TensorFlow SavedModel format in Google Drive\n",
    "saved_model_path = os.path.join(drive_path, 'saved_model')\n",
    "model.save(saved_model_path, save_format='tf')\n",
    "print(f\"Model saved in TensorFlow format at {saved_model_path}\")\n",
    "\n",
    "# Save the training and validation datasets as CSV in Google Drive\n",
    "train_csv_path = os.path.join(drive_path, 'train.csv')\n",
    "val_csv_path = os.path.join(drive_path, 'val.csv')\n",
    "train_data_provider.to_csv(train_csv_path)\n",
    "val_data_provider.to_csv(val_csv_path)\n",
    "print(f\"Training and validation datasets saved as CSV files at {train_csv_path} and {val_csv_path}\")\n",
    "\n",
    "# Save the model in ONNX format in Google Drive\n",
    "onnx_path = os.path.join(drive_path, 'model.onnx')\n",
    "model2onnx = Model2onnx(onnx_path)\n",
    "model2onnx.on_train_end(None)\n",
    "print(f\"Model saved in ONNX format at {onnx_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4164,
     "status": "ok",
     "timestamp": 1737283102679,
     "user": {
      "displayName": "Manik Chandra Biswas",
      "userId": "00149848136148089105"
     },
     "user_tz": -360
    },
    "id": "Cbh2-zrdFlvn",
    "outputId": "e744adf9-56ed-459a-94b6-0ed86d26136a"
   },
   "outputs": [],
   "source": [
    "!pip install flask-ngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 413,
     "status": "ok",
     "timestamp": 1737282915143,
     "user": {
      "displayName": "Manik Chandra Biswas",
      "userId": "00149848136148089105"
     },
     "user_tz": -360
    },
    "id": "eJBU8HphIXlR",
    "outputId": "8e9a448c-bfc1-4b02-c143-2697f0e43d72"
   },
   "outputs": [],
   "source": [
    "!ngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3973,
     "status": "ok",
     "timestamp": 1737283547888,
     "user": {
      "displayName": "Manik Chandra Biswas",
      "userId": "00149848136148089105"
     },
     "user_tz": -360
    },
    "id": "qYi_JO7gHCxQ",
    "outputId": "fec922f4-b06a-41eb-fe61-68d63d9e88e7"
   },
   "outputs": [],
   "source": [
    "!pip install flask pyngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "GbrFA-qFK61g",
    "outputId": "9ce866bb-e23e-4b3c-876d-68df052e23c9"
   },
   "outputs": [],
   "source": [
    "from flask import Flask\n",
    "from pyngrok import ngrok\n",
    "\n",
    "from flask import Flask, request, jsonify, render_template\n",
    "from PIL import Image\n",
    "from transformers import AutoTokenizer, VisionEncoderDecoderModel, GPT2Tokenizer, TFGPT2Model\n",
    "import tensorflow as tf\n",
    "from flask_ngrok import run_with_ngrok\n",
    "\n",
    "\n",
    "\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import io\n",
    "import tempfile\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=\"AIzaSyBjzFNBWDmpUmXU8t9_yof1hk4QxiF4s2E\")\n",
    "modelg = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "# response = model.generate_content(\"Explain how AI works\")\n",
    "# print(response.text)\n",
    "# Initialize Flask app\n",
    "app = Flask(__name__)\n",
    "run_with_ngrok(app)\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "# Load GPT-2 tokenizer and model\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-large\")\n",
    "gpt2_model = TFGPT2Model.from_pretrained(\"gpt2-large\")\n",
    "# Ensure the device is set\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define preprocessing for the image\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((384, 384)),  # Resize image to expected dimensions\n",
    "    transforms.ToTensor(),  # Convert PIL Image to PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize to match model expectations\n",
    "])\n",
    "\n",
    "# Define the prediction function\n",
    "def predict_text(image):\n",
    "    image = preprocess(image).unsqueeze(0).to(device)  # Add batch dimension\n",
    "    output_ids = model.generate(image)  # Generate text from the image\n",
    "    text = tokenizer.decode(output_ids[0], skip_special_tokens=True)  # Decode text\n",
    "    return text\n",
    "\n",
    "def extract_sentences(image_path):\n",
    "    # Read the image as RGB to ensure 3 channels\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "\n",
    "    # Check if the image was loaded correctly\n",
    "    if image is None:\n",
    "        raise ValueError(f\"Unable to load image at {image_path}\")\n",
    "\n",
    "    # Resize image to standardize dimensions (scaling for consistency)\n",
    "    height, width = image.shape[:2]\n",
    "    scaling_factor = 1000 / width  # Scale width to 1000 pixels\n",
    "    image = cv2.resize(image, (int(width * scaling_factor), int(height * scaling_factor)))\n",
    "\n",
    "    # Convert image to grayscale for better processing\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply GaussianBlur to reduce noise\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\n",
    "    # Use adaptive thresholding for more robust binarization\n",
    "    binary = cv2.adaptiveThreshold(\n",
    "        blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 11, 2\n",
    "    )\n",
    "\n",
    "    # Define a kernel for morphological operations\n",
    "    kernel_width = max(50, int(image.shape[1] * 0.05))  # Adjust kernel width dynamically\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (kernel_width, 1))\n",
    "\n",
    "    # Use morphological operations to detect text lines\n",
    "    detected_lines = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    # Find contours of the lines\n",
    "    contours, _ = cv2.findContours(detected_lines, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Sort contours top-to-bottom\n",
    "    contours = sorted(contours, key=lambda ctr: cv2.boundingRect(ctr)[1])\n",
    "\n",
    "    sentence_images = []  # Initialize the list to store sentence images\n",
    "\n",
    "    # Loop through each contour and extract sentences (lines of text)\n",
    "    for i, contour in enumerate(contours):\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "\n",
    "        # Filter out small boxes to avoid noise\n",
    "        if h > 10 and w > 50:  # Adjust these thresholds as needed\n",
    "            line_image = image[y:y+h, x:x+w]\n",
    "            sentence_images.append(line_image)\n",
    "\n",
    "    return sentence_images\n",
    "# Function to process multi-sentence input and return result\n",
    "def process_multisentence_image(image):\n",
    "    sentence_images = extract_sentences(image)  # Extract sentences\n",
    "    results = []\n",
    "\n",
    "    for sentence_image in sentence_images:\n",
    "        # Convert sentence image to PIL format\n",
    "        pil_image = Image.fromarray(sentence_image)\n",
    "        # Predict text for each sentence\n",
    "        text = predict_text(pil_image)\n",
    "        results.append(text)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Define GPT-2 processing function\n",
    "def process_with_gpt2(text):\n",
    "    encoded_input = gpt2_tokenizer(text, return_tensors='tf')\n",
    "    output = gpt2_model(encoded_input)\n",
    "    return output  # Return raw output for now (can refine based on use case)\n",
    "\n",
    "# Flask route to process GPT-2 text input with custom prompt\n",
    "@app.route('/process_gpt2', methods=['POST'])\n",
    "def process_gpt2():\n",
    "    data = request.get_json()\n",
    "    if not data or 'text' not in data:\n",
    "        return jsonify({'error': 'No text input provided'}), 400\n",
    "\n",
    "    text = data['text']\n",
    "    try:\n",
    "        # Process the text with GPT-2\n",
    "        gpt2_output = process_with_gpt2(text)\n",
    "        return jsonify({'gpt2_output': str(gpt2_output)})\n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    # Render HTML form for file upload and text input\n",
    "    return render_template('index.htm')\n",
    "\n",
    "\n",
    "@app.route('/extract_text', methods=['POST'])\n",
    "def extract_text():\n",
    "    if 'image' not in request.files:\n",
    "        return jsonify({'error': 'No image part'}), 400\n",
    "\n",
    "    file = request.files['image']\n",
    "    if file.filename == '':\n",
    "        return jsonify({'error': 'No selected file'}), 400\n",
    "\n",
    "    # Retrieve the custom question from the form\n",
    "    question = request.form.get('question', '')\n",
    "\n",
    "    # Save the image temporarily\n",
    "    temp_file = tempfile.NamedTemporaryFile(delete=False)\n",
    "    file_path = temp_file.name\n",
    "    file.save(file_path)\n",
    "    temp_file.close()\n",
    "\n",
    "    try:\n",
    "        # Process the image to extract sentences and predict text\n",
    "        sentence_images = extract_sentences(file_path)\n",
    "        results = [predict_text(Image.fromarray(sentence)) for sentence in sentence_images]\n",
    "        print(results)\n",
    "\n",
    "        # Join the extracted text and create the GPT-2 prompt\n",
    "        extracted_text_joined = \" \".join(results)\n",
    "        prompt = f\"{question} if this is a question and this is an answer {extracted_text_joined} then how much will you give mark\"\n",
    "        print(prompt)\n",
    "        output_text = modelg.generate_content(prompt)\n",
    "        print(output_text.text)\n",
    "\n",
    "        return jsonify({\n",
    "            'predictions': extracted_text_joined,\n",
    "            'gpt2_output': output_text.text\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "\n",
    "    finally:\n",
    "        # Clean up temporary file\n",
    "        if os.path.exists(file_path):\n",
    "            try:\n",
    "                os.remove(file_path)\n",
    "            except PermissionError:\n",
    "                print(f\"Could not delete {file_path} because it is in use.\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  ngrok.set_auth_token(\"22YNqiM4663KekUtpIqGSOPkeFt_4hA7RdtpGq8L6zfdrUUTM\")\n",
    "  ngrok_tunnel = ngrok.connect(5000)\n",
    "  print('Public URL:', ngrok_tunnel.public_url)\n",
    "  app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FrjIjBy7KVOx"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM4lQJ9X8tnxV4FCY5055Dr",
   "gpuType": "T4",
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
